{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd1cf5ef-943c-43ed-86a7-962f9210b557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What did the cat do?\n",
      "Retrieved Documents:\n",
      "- The dog chased the cat.\n",
      "- The cat sat on the mat.\n",
      "- The cat is sleeping.\n",
      "\n",
      "Generated Answer:\n",
      "Based on the context: 'The dog chased the cat. The cat sat on the mat. The cat is sleeping.', the answer to 'What did the cat do?' could be: The cat sat on the mat, chased by a dog, and played with the dog.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample documents\n",
    "corpus = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog chased the cat.\",\n",
    "    \"The cat and dog played together.\",\n",
    "    \"The cat is sleeping.\",\n",
    "    \"The dog barked loudly.\",\n",
    "]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "# Function to retrieve documents based on TF-IDF similarity\n",
    "def retrieve_documents(query, corpus, vectorizer, X, top_n=3):\n",
    "    # Transform the query using the same vectorizer\n",
    "    query_vector = vectorizer.transform([query])\n",
    "\n",
    "    # Compute cosine similarity between query and documents\n",
    "    cosine_similarities = cosine_similarity(query_vector, X).flatten()\n",
    "\n",
    "    # Get the indices of the top N similar documents\n",
    "    related_docs_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "\n",
    "    # Retrieve the documents\n",
    "    related_docs = [corpus[i] for i in related_docs_indices]\n",
    "\n",
    "    return related_docs\n",
    "\n",
    "\n",
    "# Example query\n",
    "query = \"What did the cat do?\"\n",
    "\n",
    "# Retrieve documents\n",
    "retrieved_docs = retrieve_documents(query, corpus, vectorizer, X)\n",
    "\n",
    "# Display results\n",
    "print(\"Query:\", query)\n",
    "print(\"Retrieved Documents:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(\"-\", doc)\n",
    "\n",
    "\n",
    "# Now, let's simulate a simple generation step\n",
    "def generate_answer(query, retrieved_docs):\n",
    "    # Here, we'll just concatenate the retrieved documents for simplicity\n",
    "    context = \" \".join(retrieved_docs)\n",
    "    # In a real RAG system, you would use a language model here to generate an answer based on the context\n",
    "    return f\"Based on the context: '{context}', the answer to '{query}' could be: The cat sat on the mat, chased by a dog, and played with the dog.\"\n",
    "\n",
    "\n",
    "# Generate an answer\n",
    "answer = generate_answer(query, retrieved_docs)\n",
    "print(\"\\nGenerated Answer:\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
